2025-05-08 11:09:35 - ==================== training arguments ====================
2025-05-08 11:09:35 - model: deit_small_distilled_patch16_224
2025-05-08 11:09:35 - input_size: 224
2025-05-08 11:09:35 - drop: 0.0
2025-05-08 11:09:35 - drop_path: 0.1
2025-05-08 11:09:35 - max_iter: 2000
2025-05-08 11:09:35 - batch_size: 64
2025-05-08 11:09:35 - lr: 3e-05
2025-05-08 11:09:35 - opt: adamw
2025-05-08 11:09:35 - opt_eps: 1e-08
2025-05-08 11:09:35 - opt_betas: None
2025-05-08 11:09:35 - clip_grad: None
2025-05-08 11:09:35 - momentum: 0.9
2025-05-08 11:09:35 - weight_decay: 0.0005
2025-05-08 11:09:35 - dataset: cub200
2025-05-08 11:09:35 - data_path: CUB_200_2011\CUB_200_2011
2025-05-08 11:09:35 - m: 0
2025-05-08 11:09:35 - rank: [1, 2, 4, 8]
2025-05-08 11:09:35 - num_workers: 16
2025-05-08 11:09:35 - pin_mem: True
2025-05-08 11:09:35 - lambda_reg: 0.7
2025-05-08 11:09:35 - margin: 0.5
2025-05-08 11:09:35 - memory_ratio: 1.0
2025-05-08 11:09:35 - encoder_momentum: None
2025-05-08 11:09:35 - logging_freq: 100
2025-05-08 11:09:35 - output_dir: ./outputs\cub200
2025-05-08 11:09:35 - log_dir: ./logs\cub200
2025-05-08 11:09:35 - device: cuda:0
2025-05-08 11:09:35 - seed: 0
2025-05-08 11:09:35 - ============================================================
2025-05-08 11:09:35 - Number of training examples: 5864
2025-05-08 11:09:35 - Number of query examples: 5924
2025-05-08 11:09:35 - Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth)
2025-05-08 11:09:35 - Number of params: 21.67 M
2025-05-08 11:12:02 - Iteration [  100/2,000] contrastive: 1.3684  regularization : -0.0995(x 0.7) total_loss: 1.2987  
2025-05-08 11:13:23 - Iteration [  200/2,000] contrastive: 1.1974  regularization : -0.0638(x 0.7) total_loss: 1.1527  
2025-05-08 11:14:45 - Iteration [  300/2,000] contrastive: 1.2173  regularization : -0.0791(x 0.7) total_loss: 1.1620  
2025-05-08 11:16:10 - Iteration [  400/2,000] contrastive: 1.1867  regularization : -0.0668(x 0.7) total_loss: 1.1400  
2025-05-08 11:17:33 - Iteration [  500/2,000] contrastive: 1.1108  regularization : -0.0354(x 0.7) total_loss: 1.0860  
2025-05-08 11:18:54 - Iteration [  600/2,000] contrastive: 1.1179  regularization : -0.0220(x 0.7) total_loss: 1.1024  
2025-05-08 11:20:16 - Iteration [  700/2,000] contrastive: 1.2131  regularization : -0.0599(x 0.7) total_loss: 1.1712  
2025-05-08 11:21:37 - Iteration [  800/2,000] contrastive: 1.1925  regularization : -0.0689(x 0.7) total_loss: 1.1443  
2025-05-08 11:23:01 - Iteration [  900/2,000] contrastive: 1.2034  regularization : -0.0740(x 0.7) total_loss: 1.1516  
2025-05-08 11:24:23 - Iteration [1,000/2,000] contrastive: 1.1655  regularization : -0.0557(x 0.7) total_loss: 1.1265  
2025-05-08 11:25:44 - Iteration [1,100/2,000] contrastive: 1.1641  regularization : -0.0648(x 0.7) total_loss: 1.1187  
2025-05-08 11:28:09 - Iteration [1,200/2,000] contrastive: 1.1510  regularization : -0.0524(x 0.7) total_loss: 1.1143  
2025-05-08 11:29:31 - Iteration [1,300/2,000] contrastive: 1.1214  regularization : -0.0167(x 0.7) total_loss: 1.1096  
2025-05-08 11:30:52 - Iteration [1,400/2,000] contrastive: 1.1039  regularization : -0.0069(x 0.7) total_loss: 1.0991  
2025-05-08 11:32:13 - Iteration [1,500/2,000] contrastive: 1.0858  regularization : -0.0400(x 0.7) total_loss: 1.0578  
2025-05-08 11:33:34 - Iteration [1,600/2,000] contrastive: 0.9743  regularization : -0.0335(x 0.7) total_loss: 0.9508  
2025-05-08 11:34:55 - Iteration [1,700/2,000] contrastive: 1.0412  regularization : -0.0412(x 0.7) total_loss: 1.0123  
2025-05-08 11:36:16 - Iteration [1,800/2,000] contrastive: 1.0585  regularization : -0.0237(x 0.7) total_loss: 1.0420  
2025-05-08 11:37:37 - Iteration [1,900/2,000] contrastive: 1.0310  regularization : -0.0120(x 0.7) total_loss: 1.0225  
2025-05-08 11:38:58 - Iteration [2,000/2,000] contrastive: 1.1407  regularization : -0.0493(x 0.7) total_loss: 1.1062  
2025-05-08 11:39:00 - Start evaluation job
2025-05-08 11:40:11 - Recall@1 : 76.16%
2025-05-08 11:40:11 - Recall@2 : 85.15%
2025-05-08 11:40:11 - Recall@4 : 90.88%
2025-05-08 11:40:11 - Recall@8 : 94.43%
2025-05-08 11:40:11 - Training time 0:30:35
1_logging_cub_koleoknn
2_logging_sop_nguyenthuy
2_logging_sop_koleoknn

