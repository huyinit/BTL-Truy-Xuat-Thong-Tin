2025-05-08 11:51:23 - ==================== training arguments ====================
2025-05-08 11:51:23 - model: deit_small_distilled_patch16_224
2025-05-08 11:51:23 - input_size: 224
2025-05-08 11:51:23 - drop: 0.0
2025-05-08 11:51:23 - drop_path: 0.1
2025-05-08 11:51:23 - max_iter: 2000
2025-05-08 11:51:23 - batch_size: 64
2025-05-08 11:51:23 - lr: 3e-05
2025-05-08 11:51:23 - opt: adamw
2025-05-08 11:51:23 - opt_eps: 1e-08
2025-05-08 11:51:23 - opt_betas: None
2025-05-08 11:51:23 - clip_grad: None
2025-05-08 11:51:23 - momentum: 0.9
2025-05-08 11:51:23 - weight_decay: 0.0005
2025-05-08 11:51:23 - dataset: sop
2025-05-08 11:51:23 - data_path: Stanford_Online_Products
2025-05-08 11:51:23 - m: 2
2025-05-08 11:51:23 - rank: [1, 10, 100, 1000]
2025-05-08 11:51:23 - num_workers: 16
2025-05-08 11:51:23 - pin_mem: True
2025-05-08 11:51:23 - lambda_reg: 0.7
2025-05-08 11:51:23 - margin: 0.5
2025-05-08 11:51:23 - memory_ratio: 1.0
2025-05-08 11:51:23 - encoder_momentum: None
2025-05-08 11:51:23 - logging_freq: 100
2025-05-08 11:51:23 - output_dir: ./outputs\sop
2025-05-08 11:51:23 - log_dir: ./logs\sop
2025-05-08 11:51:23 - device: cuda:0
2025-05-08 11:51:23 - seed: 0
2025-05-08 11:51:23 - ============================================================
2025-05-08 11:51:23 - Number of training examples: 59551
2025-05-08 11:51:23 - Number of query examples: 60502
2025-05-08 11:51:23 - Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth)
2025-05-08 11:51:23 - Number of params: 21.67 M
2025-05-08 11:52:46 - Iteration [  100/2,000] contrastive: 1.0718  regularization : 0.0203(x 0.7) total_loss: 1.0860  
2025-05-08 11:53:05 - Iteration [  200/2,000] contrastive: 1.0251  regularization : 0.0268(x 0.7) total_loss: 1.0438  
2025-05-08 11:53:23 - Iteration [  300/2,000] contrastive: 0.9534  regularization : 0.0235(x 0.7) total_loss: 0.9699  
2025-05-08 11:53:41 - Iteration [  400/2,000] contrastive: 0.9805  regularization : 0.0779(x 0.7) total_loss: 1.0351  
2025-05-08 11:54:00 - Iteration [  500/2,000] contrastive: 1.1201  regularization : 0.0099(x 0.7) total_loss: 1.1270  
2025-05-08 11:54:18 - Iteration [  600/2,000] contrastive: 0.9856  regularization : 0.0337(x 0.7) total_loss: 1.0092  
2025-05-08 11:54:36 - Iteration [  700/2,000] contrastive: 1.0271  regularization : 0.0255(x 0.7) total_loss: 1.0449  
2025-05-08 11:54:55 - Iteration [  800/2,000] contrastive: 0.9697  regularization : 0.0754(x 0.7) total_loss: 1.0225  
2025-05-08 11:55:14 - Iteration [  900/2,000] contrastive: 0.9840  regularization : 0.0445(x 0.7) total_loss: 1.0151  
2025-05-08 11:55:32 - Iteration [1,000/2,000] contrastive: 0.9756  regularization : 0.0410(x 0.7) total_loss: 1.0043  
2025-05-08 11:55:51 - Iteration [1,100/2,000] contrastive: 1.0184  regularization : 0.0633(x 0.7) total_loss: 1.0627  
2025-05-08 11:56:10 - Iteration [1,200/2,000] contrastive: 0.9473  regularization : 0.0762(x 0.7) total_loss: 1.0006  
2025-05-08 11:56:29 - Iteration [1,300/2,000] contrastive: 0.9861  regularization : 0.0343(x 0.7) total_loss: 1.0102  
2025-05-08 11:56:47 - Iteration [1,400/2,000] contrastive: 0.9864  regularization : 0.0417(x 0.7) total_loss: 1.0156  
2025-05-08 11:57:06 - Iteration [1,500/2,000] contrastive: 1.0023  regularization : 0.0131(x 0.7) total_loss: 1.0115  
2025-05-08 11:58:30 - Iteration [1,600/2,000] contrastive: 0.9179  regularization : 0.0687(x 0.7) total_loss: 0.9660  
2025-05-08 11:58:48 - Iteration [1,700/2,000] contrastive: 1.0233  regularization : 0.0413(x 0.7) total_loss: 1.0522  
2025-05-08 11:59:07 - Iteration [1,800/2,000] contrastive: 0.9464  regularization : 0.0751(x 0.7) total_loss: 0.9990  
2025-05-08 11:59:25 - Iteration [1,900/2,000] contrastive: 0.9377  regularization : 0.0774(x 0.7) total_loss: 0.9918  
2025-05-08 11:59:44 - Iteration [2,000/2,000] contrastive: 0.9361  regularization : 0.0647(x 0.7) total_loss: 0.9814  
2025-05-08 11:59:46 - Start evaluation job
2025-05-08 12:02:12 - Recall@1 : 75.34%
2025-05-08 12:02:12 - Recall@10 : 88.23%
2025-05-08 12:02:12 - Recall@100 : 94.71%
2025-05-08 12:02:12 - Recall@1000 : 98.05%
2025-05-08 12:02:12 - Training time 0:10:48
